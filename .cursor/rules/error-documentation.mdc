---
description: Document major failure points in this project and they were solved.  To be filled by AI.
globs: 
alwaysApply: false
---
---
description: Document major failure points in this project and they were solved.  To be filled by AI. 
globs: 
---

## CRITICAL: Path Management in Scripts

### Issue: ALWAYS USE ABSOLUTE PATHS IN SCRIPTS 
**Error**: `./run_dashboard.sh: line 61: ./stop_dashboard.sh: No such file or directory`

**Root Cause**: 
- Scripts using relative paths (`./script.sh`) fail when the working directory changes during execution
- When `cd` commands are used in scripts, relative paths break because they're resolved from the new directory

**MANDATORY SOLUTION**:
1. ALWAYS capture the root directory at the start of EVERY script:
```bash
# Store the root directory path - ADD THIS TO EVERY SCRIPT
ROOT_DIR="$(pwd)"
```

2. ALWAYS use the full path with $ROOT_DIR for ALL script calls:
```bash
# CORRECT - Will work from any directory
"$ROOT_DIR/stop_dashboard.sh"

# WRONG - Will fail if current directory changes
./stop_dashboard.sh
```

3. After using `cd` commands, ALWAYS reference scripts using the ROOT_DIR path:
```bash
# Change directory
cd some/other/dir

# Then use full path for any script calls
"$ROOT_DIR/stop_dashboard.sh"

# Return to the root directory when needed
cd "$ROOT_DIR"
```

4. ALWAYS use full path checks in conditional statements:
```bash
if [ -f "$ROOT_DIR/stop_dashboard.sh" ]; then
    "$ROOT_DIR/stop_dashboard.sh"
else
    echo "Script not found at: $ROOT_DIR/stop_dashboard.sh"
fi
```

## Dashboard Startup Failures

### Issue: Dashboard Fails to Start with Missing stop_dashboard.sh
**Error**: `./run_dashboard.sh: line 61: ./stop_dashboard.sh: No such file or directory`

**Root Cause**: 
- The `run_dashboard.sh` script attempts to call `./stop_dashboard.sh` when an error occurs
- The stop_dashboard.sh script exists but can't be found because the working directory changed

**Solution**:
1. ALWAYS use absolute paths with ROOT_DIR variable for all script calls:
```bash
# Store the root directory path at the beginning of the script
ROOT_DIR="$(pwd)"

# Use the full path for all script calls
"$ROOT_DIR/stop_dashboard.sh"
```

2. Add checks to handle cases where scripts might not exist:
```bash
if [ -f "$ROOT_DIR/stop_dashboard.sh" ]; then
    "$ROOT_DIR/stop_dashboard.sh"
else
    echo "Warning: stop_dashboard.sh not found at $ROOT_DIR/stop_dashboard.sh. Attempting manual cleanup..."
    # Manual cleanup code here
fi
```

3. Always restore the working directory after changing it:
```bash
cd web && ./run_direct.sh &
DASHBOARD_PID=$!

# Immediately return to root directory
cd "$ROOT_DIR"
```

### Issue: Backend Server Fails to Start
**Error**: Backend process terminates during startup

**Root Cause**:
- Missing dependencies or environment setup
- Incorrect activation of virtual environment
- Potential import errors in Python modules

**Solution**:
1. Always run the backend from its specific directory with the correct virtual environment:
```bash
cd "$ROOT_DIR/web/backend"
source venv/bin/activate
python run.py
```
2. Update the `run_dashboard.sh` script to properly handle the virtual environment setup
3. Add more verbose error logging during startup
4. Add import validation checks before attempting to start the server

### Issue: Virtual Environment Confusion
**Error**: Missing dependencies despite being installed

**Root Cause**:
- Multiple virtual environments cause confusion
- Wrong virtual environment being activated

**Solution**:
1. Clearly document which virtual environment is used for which component
2. Add validation in scripts to confirm correct environment is active
3. Consider consolidating to fewer virtual environments if possible

## Video Streaming Issues

### Issue: "Media Container Not Supported" Error in DLNA Streaming
**Error**: DLNA devices report "Media container not supported" when trying to play videos through the dashboard

**Root Cause**: 
1. DLNA devices make TWO sequential HTTP requests when streaming a video:
   - First request: For the video file itself
   - Second request: For container format information and DLNA capabilities
2. The second request was failing with 404 Not Found because:
   - The SimpleHTTPRequestHandler didn't maintain "memory" between requests
   - The URLs in the second request often have slight variations from the first
   - Insufficient DLNA-specific HTTP headers required by devices
   - Inadequate DIDL-Lite metadata in SOAP requests

**Solution**:
1. Implemented a Twisted-based streaming server (web/backend/core/twisted_streaming.py) 
2. Used the Twisted File server which properly handles MIME types and DLNA dual-request pattern
3. Enhanced the streaming server to:
   - Add proper DLNA-specific HTTP headers (contentFeatures.dlna.org, transferMode.dlna.org)
   - Provide reliable file access between sequential requests
   - Include complete DIDL-Lite metadata with proper DLNA profiles
4. Updated the DLNA device implementation to use the Twisted server instead of SimpleHTTPRequestHandler

**Key Lesson**:
The CLI tool version works because it uses Twisted's File server with proper MIME handling, while the Dashboard was originally using SimpleHTTPRequestHandler which doesn't maintain file accessibility between the sequential requests that DLNA devices make. Aligning the dashboard implementation with the CLI's approach resolved the issue.

**References**:
- Original implementation: nanodlna/streaming.py (CLI version using Twisted)
- Fixed implementation: web/backend/core/twisted_streaming.py (Dashboard version using Twisted)
- Modified DLNA device: web/backend/core/dlna_device.py

## Device Status and Playback Control Issues

### Issue: Unreliable Device Status Tracking
**Error**: Devices showing incorrect status (connected/disconnected) or playback state

**Root Cause**:
- Lack of thread safety in status updates
- Missing validation of device state
- No comprehensive health monitoring
- Insufficient cleanup during state changes

**Solution**:
1. Implemented thread-safe status tracking:
```python
def update_device_status(self, device_name: str, status: str, is_playing: bool = None):
    with self.status_lock:
        if device_name not in self.device_status:
            self.device_status[device_name] = {}
        status_dict = self.device_status[device_name]
        status_dict["status"] = status
        status_dict["last_updated"] = time.time()
        if is_playing is not None:
            status_dict["is_playing"] = is_playing
```

2. Added health check monitoring:
```python
def _playback_health_check_loop(self, device_name: str, video_path: str):
    while True:
        # Check device status
        device = self.get_device(device_name)
        if not device.is_playing:
            # Attempt recovery
            self.auto_play_video(device, video_path, loop=True)
```

3. Implemented proper cleanup:
```python
def unregister_device(self, device_name: str):
    with self.device_lock:
        # Clean up device tracking
        if device_name in self.devices:
            del self.devices[device_name]
        # Clean up status tracking
        with self.status_lock:
            if device_name in self.device_status:
                del self.device_status[device_name]
```

### Issue: Video Assignment Conflicts
**Error**: Multiple videos being assigned to the same device or incorrect video playing

**Root Cause**:
- No priority system for video assignments
- Missing validation before assignments
- Insufficient cleanup of previous assignments
- Race conditions in assignment process

**Solution**:
1. Implemented priority-based assignment:
```python
def assign_video_to_device(self, device_name: str, video_path: str, priority: int = 50):
    with self.video_assignment_lock:
        current_priority = self.video_assignment_priority.get(device_name, 0)
        if priority >= current_priority:
            # Proceed with assignment
            self.video_assignment_priority[device_name] = priority
```

2. Added proper cleanup before new assignments:
```python
def auto_play_video(self, device: Device, video_path: str, loop: bool = True):
    # Stop any current playback
    if device.is_playing:
        device.stop()
        time.sleep(1)  # Give it time to stop
```

3. Implemented retry logic with exponential backoff:
```python
def _schedule_retry(self, device_name: str, video_path: str, priority: int):
    retry_count = self.video_assignment_retries.get(device_name, 0)
    if retry_count < MAX_RETRY_ATTEMPTS:
        delay = RETRY_DELAY_BASE * (2 ** retry_count)
        retry_timer = threading.Timer(delay, self.assign_video_to_device,
                                    args=[device_name, video_path, priority])
        retry_timer.start()
```

**Key Lessons**:
1. Always use thread-safe operations for status updates
2. Implement comprehensive health monitoring
3. Use priority system for resolving conflicts
4. Add proper cleanup during state changes
5. Implement retry logic with exponential backoff

## Device Status Desynchronization (May 2025)

**Issue:** Devices not present on the network remained marked as "connected" in the dashboard, causing confusion and inaccurate device management.

**Root Cause:** The /api/devices/discover endpoint did not update the status of missing devices, only adding/updating found devices.

**Resolution:** Added a sync step after discovery to mark missing devices as "disconnected" in both the DB and in-memory. Now, the device list always matches the real network state after discovery.

**Lesson:** Always ensure device discovery endpoints update the status of all devices, not just those found, to prevent stale state in the dashboard.

## Backend Attribute Errors (May 2025)

### Issue: Missing discovery_interval attribute in DeviceManager
**Error:** `AttributeError: 'DeviceManager' object has no attribute 'discovery_interval'. Did you mean: 'discovery_thread'?`

**Root Cause:** The DeviceManager class was missing the discovery_interval attribute in its initialization, but this attribute was being referenced in the _discovery_loop method.

**Resolution:**
1. Added the missing discovery_interval attribute to the DeviceManager.__init__ method:
```python
def __init__(self):
    # Other initializations...
    self.discovery_interval = 10  # Seconds between discovery cycles
```

**Lesson:** Always ensure that all attributes used in class methods are properly initialized in the __init__ method, especially for long-running threaded operations.

### Issue: Incorrect method name in streaming service shutdown
**Error:** `AttributeError: 'TwistedStreamingServer' object has no attribute 'stop_all_servers'. Did you mean: 'stop_server'?`

**Root Cause:** In main.py's shutdown_event handler, there was a call to streaming_service.stop_all_servers(), but the actual method name in the TwistedStreamingServer class is stop_server().

**Resolution:**
1. Updated the method call in the shutdown_event handler:
```python
@app.on_event("shutdown")
async def shutdown_event():
    # Other shutdown code...
    streaming_service.stop_server()  # Changed from stop_all_servers
```

**Lesson:** Always verify method names when calling external modules, especially during critical operations like application shutdown. Using IDE features like code completion can help identify correct method names.

## Device Status Tracking Issues

### Problem

Devices would appear as connected in the dashboard even when they were disconnected from the network. Additionally, device discovery would restart video playback on devices that were already playing videos.

### Solution

We implemented several fixes for these issues:

1. **Device Status Synchronization**: We modified the `sync_device_status_with_discovery` method in `DeviceService` to properly update device status based on discovery results. Devices not found in the latest discovery are marked as "disconnected".

2. **Selective Auto-Play Logic**: We completely redesigned the device discovery flow to:
   - Keep a clear distinction between new and existing devices
   - Only update connection status of existing devices, not their entire configuration
   - Only attempt auto-play on newly discovered devices, not existing ones
   - Check multiple sources to determine if a device is already playing (streaming registry, core device, database status, current_video field)
   - Use proper locking around critical sections of code
   - Add comprehensive error handling and logging

3. **API Endpoint Improvements**: We updated the discovery endpoint to support both GET and POST methods for better compatibility with different clients.

The solution is in `web/backend/services/device_service.py`, lines 300-396.

## DLNA Media Container Issues

## DLNA Loop Monitoring Thread Errors

### Issue: Missing current_video_duration Attribute in DLNADevice
**Error**: `AttributeError: 'DLNADevice' object has no attribute 'current_video_duration'`

**Root Cause**:
- The `_monitor_and_loop` method in `DLNADevice` class was trying to access a `current_video_duration` attribute that wasn't initialized in the `__init__` method
- The loop monitoring thread would crash immediately upon starting, preventing proper video looping functionality
- This caused videos to play once but not loop continuously as expected

**Solution**:
1. Added the missing attribute initialization in the `__init__` method:
```python
def __init__(self, device_info: Dict[str, Any]):
    # Other initializations...
    
    # Video playback attributes
    self.current_video_duration = None  # Duration of current video in seconds
    self._looping = False  # Flag to indicate if video should loop
```

2. Improved the `_monitor_and_loop` method to handle cases where the attribute might not exist:
```python
def _monitor_and_loop(self, video_url: str) -> None:
    # Initialize video duration if not already set
    if not hasattr(self, 'current_video_duration') or self.current_video_duration is None:
        self.current_video_duration = None
        
    # Keep trying to get the video duration with retry limit
    retry_count = 0
    while self._looping and (self.current_video_duration is None) and retry_count < 10:
        # Attempt to get duration...
        retry_count += 1
```

**Key Lessons**:
1. Always initialize all attributes in the `__init__` method that will be used elsewhere in the class
2. Add defensive programming with attribute existence checks using `hasattr()` for critical operations
3. Implement retry limits to prevent infinite loops when external data can't be retrieved
4. Add proper error handling in threaded operations to prevent silent failures

## Port Conflict and Thread Safety Issues in DLNA Streaming

### Issue: Port Conflict in Twisted Streaming Server
**Error**: `Error starting streaming server: Couldn't listen on 10.0.0.74:8000: [Errno 48] Address already in use.`

**Root Cause**:
- The streaming server was trying to use a fixed port (8000) that was already in use
- Multiple streaming servers were being started without properly cleaning up previous instances
- The port selection logic wasn't properly using the configured port range

**Solution**:
1. Ensured consistent use of port_range parameter across all components:
   - Added missing Tuple import in dlna_device.py
   - Updated streaming_service.py to use Tuple[int, int] type annotation for port_range
   - Made port range handling consistent across all components

2. Improved port selection logic in TwistedStreamingServer:
   - Added better port availability checking before attempting to bind
   - Implemented proper fallback to try multiple ports in the configured range
   - Added more detailed logging of port selection attempts

### Issue: NoneType Error in Thread Monitoring
**Error**: `Error playing video on Hccast-3ADE76_dlna: 'NoneType' object has no attribute 'is_alive'`

**Root Cause**:
- In the _setup_loop_monitoring method, there was insufficient null checking before accessing the is_alive attribute of the thread object
- The thread object could be None in some cases, causing the error

**Solution**:
1. Added robust null checking in the _setup_loop_monitoring method:
   ```python
   thread_is_running = False
   if self._loop_thread is not None:
       try:
           # First check if the thread object exists and has the is_alive attribute
           if hasattr(self._loop_thread, 'is_alive'):
               # Then check if the thread is alive
               thread_is_running = self._loop_thread.is_alive()
               if thread_is_running:
                   logger.info(f"[{self.name}] Loop monitoring thread already running")
                   return
       except (AttributeError, TypeError) as e:
           logger.warning(f"[{self.name}] Error checking thread status: {e}")
           # Reset thread to None if there was an error
           self._loop_thread = None
   ```

2. This approach:
   - First checks if the thread object exists
   - Then checks if it has the is_alive attribute
   - Only then tries to access the is_alive method
   - Handles any exceptions that might occur during the check
   - Resets the thread to None if there's an error

**Key Lessons**:
1. Always use defensive programming when dealing with thread objects
2. Implement proper null checking before accessing attributes
3. Use port ranges instead of fixed ports for network services
4. Handle port conflicts gracefully with fallback options

## Import Path Issues in Depth Processing Module

### Problem
The depth processing module had import path issues causing the server to fail to start. The error message was:
```
ModuleNotFoundError: No module named 'numpy'
```

The actual issue was that the imports in the depth processing module were using absolute imports (e.g., `from core.depth_processing.core.depth_loader import DepthLoader`) which were causing import failures when Python attempted to resolve the module paths.

### Solution
Changed all imports to use relative imports:
1. In web/backend/core/depth_processing/__init__.py:
   ```python
   from .core.depth_loader import DepthLoader
   from .core.segmentation import DepthSegmenter
   from .utils.visualizer import DepthVisualizer
   ```

2. In web/backend/core/depth_processing/core/__init__.py:
   ```python
   from .depth_loader import DepthLoader
   from .segmentation import DepthSegmenter
   ```

3. In web/backend/core/depth_processing/utils/__init__.py:
   ```python
   from .visualizer import DepthVisualizer
   ```

4. In web/backend/routers/depth_router.py, simplified imports:
   ```python
   from core.depth_processing import DepthLoader, DepthSegmenter, DepthVisualizer
   ```

5. In web/backend/core/depth_processing/ui/depth_segmentation_app.py:
   ```python
   from core.depth_processing import DepthLoader, DepthSegmenter, DepthVisualizer
   ```

### Outcome
The server was able to start successfully with the depth_router included. All API endpoints are now accessible. The depth processing functionality needs further testing to confirm it's working as expected.

### Root Cause Analysis
The import issue was likely due to how Python resolves module paths. When using absolute imports that start with a package name, Python looks for that package in the sys.path. With the way the project is structured, the 'core' module is part of the 'web/backend' package and not directly in the Python path. 

Relative imports properly resolve based on the current module's location in the package hierarchy.

## 500 Internal Server Error on Device Play API

### Problem
When attempting to play a video on a DLNA device, the API returns a 500 Internal Server Error. The error can be seen in the dashboard_run.log:
```
[get_device_instance] Looking for device 'Hccast-3ADE76_dlna' in DeviceManager
[get_device_instance] DeviceManager.devices keys BEFORE: ['Smart_Projector-45[DLNA]', 'SideProjector_dlna', 'Hccast-3ADE76_dlna', 'AS-Projector[DMR]']
[get_device_instance] Found device 'Hccast-3ADE76_dlna' in DeviceManager
INFO:     127.0.0.1:0 - "POST /api/devices/1/play HTTP/1.1" 500 Internal Server Error
```

### Initial Analysis
The device is found in the DeviceManager, but the play command is failing. This could be due to:
1. Issues with the DLNA communication
2. Video format incompatibility
3. Network connectivity problems
4. Error in the play implementation

### Next Steps for Resolution
1. Check detailed logs for the specific error message
2. Examine the play_video method in device_service.py
3. Debug the DLNA device implementation
4. Test with different video formats and devices

This issue requires further investigation.

## Backend Import Errors

### Issue: Missing Python Modules in Backend
**Error**: `ModuleNotFoundError: No module named 'fastapi'`

**Root Cause**:
- The backend's virtual environment exists but is not being properly activated when running the dashboard
- This causes Python to look for packages in the system Python environment instead of the virtual environment
- Even though the packages are installed in the virtual environment, they're not accessible to the backend process

**Solution**:
1. Created a script to properly set up and verify the backend environment:
```bash
#!/bin/bash
# Store the root directory path
ROOT_DIR="$(pwd)"

# Navigate to the backend directory
cd "$ROOT_DIR/web/backend"

# Activate virtual environment
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Verify fastapi is installed
pip list | grep fastapi
```

2. Run the script to ensure the virtual environment is properly set up:
```bash
chmod +x fix_backend_env.sh
./fix_backend_env.sh
```

3. After running the script, start the dashboard normally:
```bash
./run_dashboard.sh
```

**Key Lessons**:
1. Always verify that virtual environments are properly activated before running Python applications
2. Create helper scripts to set up and verify environments to make troubleshooting easier
3. Check for module import errors in the logs as they often indicate environment setup issues rather than code issues

## Renderer Service and AirPlay Issues

### Problem: AirPlay Discovery Not Working in Frontend
**Issue**: The AirPlay discovery feature in the frontend has a cast symbol but doesn't do anything when clicked. The renderer page doesn't function as expected.

**Investigation Findings**:
1. When attempting to use the Chrome renderer to play door6.mp4 via the API, we found the following error in the logs:
```
2025-05-09 20:05:55,572 - core.dlna_device - ERROR - Error playing video on Hccast-3ADE76_dlna: 'NoneType' object has no attribute 'is_alive'
2025-05-09 20:05:55,572 - core.device_manager - ERROR - Failed to play video on device Hccast-3ADE76_dlna
```

2. The system was trying to play the video on a DLNA device (Hccast-3ADE76_dlna) rather than using Chrome renderer as expected.

3. The logs showed that the device was registered and the system attempted to auto-play the video:
```
2025-05-09 20:05:54,795 - core.device_manager - INFO - Registered DLNA device: Hccast-3ADE76_dlna with action URL: http://10.0.0.154:49595/upnp/control/rendertransport1
2025-05-09 20:05:54,796 - core.device_manager - INFO - Assigning video /Users/mannybhidya/PycharmProjects/nano-dlna/door6.mp4 to device Hccast-3ADE76_dlna
```

4. The error occurred in the loop monitoring setup, suggesting an issue with the playback monitoring thread.

**Root Cause Analysis**:
1. The 'NoneType' object has no attribute 'is_alive' error indicates that a thread or process that should be monitoring the playback is not properly initialized.
2. The AirPlay discovery in the frontend is not properly connected to the backend API endpoints.
3. There appears to be confusion in the system between using DLNA and AirPlay/Chrome for rendering.
4. In the `RendererService.start_renderer` method, when a projector is configured to use AirPlay or DLNA as the sender, it still creates and starts a Chrome renderer. However, for DLNA devices, it then tries to send the content to the DLNA device directly using the device's `play` method, which bypasses the Chrome renderer entirely.
5. The "proj-hccast" projector is configured to use AirPlay as the sender in renderer_config.json, but the test script is trying to use it with a Chrome renderer. This mismatch is causing confusion in the system.
6. The renderer service is initialized with an empty files dictionary and a default IP address of 127.0.0.1, which might not be accessible from external devices.

**Potential Solutions**:
1. Fix the thread initialization in the DLNA device implementation to ensure the monitoring thread is properly created before checking its status.
2. Review the frontend implementation of the AirPlay discovery to ensure it's properly calling the backend API.
3. Ensure the renderer service correctly distinguishes between DLNA and AirPlay/Chrome rendering methods.
4. Add better error handling in the playback monitoring to prevent NoneType errors.
5. Update the renderer_config.json to ensure the "proj-hccast" projector is configured correctly - either change the sender type to match the test script or update the test script to use the correct sender type.
6. Modify the RendererService initialization to use the correct IP address for the streaming server.
7. Implement proper error handling in the start_renderer method to handle cases where the sender type doesn't match the expected renderer type.

**Troubleshooting Steps**:
1. Check the logs for renderer-related entries to identify when and how the renderer service is initialized.
2. Examine the implementation of the loop monitoring in the DLNA device class.
3. Test the renderer API endpoints directly using curl to bypass the frontend.
4. Review the AirPlay discovery implementation in both frontend and backend.
5. Update the renderer_config.json to ensure consistent configuration.
6. Add more detailed logging in the RendererService to track the flow of execution.
